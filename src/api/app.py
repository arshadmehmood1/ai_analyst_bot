import logging
from datetime import datetime
from fastapi import FastAPI, HTTPException, status, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, RootModel
from typing import Dict, Any, List, Optional, Set
import pandas as pd

# Assuming correct relative imports from the project structure
from src.bot.assistant import get_assistant
from src.analysis.analyzer import get_analyzer
from src.config.settings import settings

logger = logging.getLogger(__name__)

# --- Setup ---
app = FastAPI(
    title="Advanced Gemini Complaint Assistant API",
    description="Enhanced RAG and Analysis service with date ranges, overtime, and statistics.",
    version="2.0.0"
)

# CORS Middleware Configuration - MUST BE FIRST
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://127.0.0.1:8000",
        "http://localhost:8000",
        "http://127.0.0.1",
        "http://localhost",
        "*"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
    max_age=600,
)

# Configure logging
logging.basicConfig(level=settings.LOGGING_LEVEL)


# --- Pydantic Models ---

class QueryRequest(BaseModel):
    """Model for the RAG query request."""
    question: str = Field(..., description="The user's question or complaint summary.")

class QueryResponse(BaseModel):
    """Model for the RAG query response."""
    answer: str = Field(..., description="The answer generated by the LLM.")
    sources: List[Dict[str, Any]] = Field(..., description="The source documents used for grounding.")
    metadata: Dict[str, Any] = Field(..., description="Run metadata (e.g., model name, source count).")

class AnalysisSummaryResponse(BaseModel):
    """Model for the statistical summary response."""
    total_complaints: int
    categories_breakdown: Dict[str, int]
    status: str

class StatisticsResponse(BaseModel):
    """Model for comprehensive statistics response."""
    total_complaints: int
    open_complaints: int
    overtime_complaints: int
    average_resolution_time: str
    min_resolution_time: Optional[str] = None
    max_resolution_time: Optional[str] = None
    median_resolution_time: Optional[str] = None
    by_category: Dict[str, int]
    by_status: Dict[str, int]

class ResolutionTimeResponse(BaseModel):
    """Model for resolution time analysis."""
    average_resolution_time: str
    min_resolution_time: str
    max_resolution_time: str
    median_resolution_time: Optional[str] = None
    total_resolved: int

class ComplaintRecord(RootModel[Dict[str, Any]]):
    """Model for a single complaint record."""
    pass

class VectorSearchRequest(BaseModel):
    """Model for vector search request."""
    query: str
    top_k: int = 5

class VectorSearchResponse(BaseModel):
    """Model for vector search response."""
    results: List[Dict[str, Any]]

class GenerateReportRequest(BaseModel):
    """Model for report generation request."""
    category: str
    records: List[Dict[str, Any]]


# --- Helper Functions ---

def parse_date(date_str: str) -> Optional[datetime]:
    """Parse date string in YYYY-MM-DD format"""
    try:
        return datetime.strptime(date_str, "%Y-%m-%d")
    except:
        return None

def calculate_resolution_time(row: Dict[str, Any]) -> Optional[float]:
    """Calculate resolution time in days"""
    try:
        # Try multiple possible column names for start date
        start_date = row.get('date_of_issue') or row.get('date_entry') or \
                     row.get('Entry Date') or row.get('entry_date') or \
                     row.get('Date of Issue')
        
        # Try multiple possible column names for end date
        end_date = row.get('completed_date') or row.get('closed_date') or \
                   row.get('rca_date') or row.get('capa_date') or \
                   row.get('Completed Date') or row.get('Resolved Date') or \
                   row.get('resolved_date')
        
        if not start_date or not end_date:
            return None
            
        entry = pd.to_datetime(start_date, errors='coerce')
        resolved = pd.to_datetime(end_date, errors='coerce')
        
        if pd.isna(entry) or pd.isna(resolved):
            return None
            
        return (resolved - entry).days
    except Exception as e:
        logger.debug(f"Error calculating resolution time: {e}")
        return None

def is_overtime(row: Dict[str, Any], sla_days: int = 30) -> bool:
    """Check if complaint exceeded SLA"""
    resolution_time = calculate_resolution_time(row)
    if resolution_time is None:
        return False
    return resolution_time > sla_days


# --- Startup Event ---

@app.on_event("startup")
async def startup_event():
    """Initialize assistants on startup to load models and data once."""
    logger.info("Initializing RAG assistant and Data Analyzer...")
    get_assistant()
    get_analyzer()
    logger.info("Startup initialization complete.")


# --- Health Check ---

@app.get("/analysis/debug/columns", tags=["Debug"])
def debug_columns():
    """
    Debug endpoint to show all available columns and sample data.
    """
    analyzer = get_analyzer()
    df = analyzer.df
    
    if df is None or df.empty:
        return {"error": "No data loaded"}
    
    return {
        "total_rows": len(df),
        "columns": list(df.columns),
        "sample_row": df.iloc[0].to_dict() if len(df) > 0 else {},
        "dtypes": df.dtypes.to_dict()
    }

@app.get("/", tags=["Health"])
def read_root():
    """Basic health check."""
    return {
        "message": "Advanced Gemini Complaint Assistant API is running!",
        "model": settings.MODEL_NAME,
        "version": "2.0.0",
        "endpoints": [
            "/query",
            "/analysis/summary",
            "/analysis/statistics",
            "/analysis/complaint/{id}",
            "/analysis/branch/{code}",
            "/analysis/status/{status}",
            "/analysis/assignee/{name}",
            "/analysis/filter",
            "/analysis/date-range",
            "/analysis/overtime",
            "/analysis/resolution-time",
            "/analysis/all-data",
            "/analysis/columns/{column_name}",
            "/vector/search"
        ]
    }


# --- RAG Query Endpoints ---

@app.post("/query", response_model=QueryResponse, tags=["RAG"])
def query_rag_assistant(request: QueryRequest):
    """
    Runs a user question through the RAG pipeline to generate a grounded answer.
    """
    assistant = get_assistant()
    try:
        response = assistant.query(request.question)
        return QueryResponse(
            answer=response["answer"],
            sources=response["source_documents"],
            metadata=response["metadata"]
        )
    except Exception as e:
        logger.error(f"RAG Query failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal RAG processing error: {e}"
        )

@app.post("/vector/search", response_model=VectorSearchResponse, tags=["RAG"])
def vector_search(request: VectorSearchRequest):
    """
    Perform vector search for RAG functionality.
    Returns top-k most similar documents.
    """
    assistant = get_assistant()
    analyzer = get_analyzer()
    
    try:
        # Try to use the assistant's retriever if available
        if hasattr(assistant, 'retriever') and assistant.retriever:
            docs = assistant.retriever.get_relevant_documents(request.query)
            results = []
            for i, doc in enumerate(docs[:request.top_k]):
                results.append({
                    "id": str(i),
                    "text": doc.page_content,
                    "score": getattr(doc, 'score', 0.8),
                    "metadata": doc.metadata
                })
            return VectorSearchResponse(results=results)
        
        # Fallback: Simple keyword search
        df = analyzer.df
        if df is None or df.empty:
            return VectorSearchResponse(results=[])
        
        query_lower = request.query.lower()
        results = []
        
        for idx, row in df.iterrows():
            row_text = " ".join([str(val) for val in row.values if pd.notna(val)]).lower()
            
            if query_lower in row_text:
                results.append({
                    "id": str(row.get("Complaint ID", idx)),
                    "text": row_text[:200],
                    "score": 0.8,
                    "metadata": row.to_dict()
                })
                
                if len(results) >= request.top_k:
                    break
        
        return VectorSearchResponse(results=results)
        
    except Exception as e:
        logger.error(f"Vector search failed: {e}")
        return VectorSearchResponse(results=[])


# --- Analysis Endpoints (Existing) ---

@app.get("/analysis/summary", response_model=AnalysisSummaryResponse, tags=["Analysis"])
def get_complaint_summary():
    """
    Provides overall statistical analysis of all complaints.
    """
    analyzer = get_analyzer()
    summary = analyzer.get_summary_stats()
    return summary

@app.get("/analysis/complaint/{complaint_id}", response_model=ComplaintRecord, tags=["Analysis"])
def get_complaint_by_id(complaint_id: str):
    """
    Fetches complete details of a single complaint by ID.
    """
    analyzer = get_analyzer()
    record = analyzer.get_complaint_by_id(complaint_id)
    
    if not record:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Complaint ID '{complaint_id}' not found."
        )
    
    return record

@app.get("/analysis/branch/{branch_code}", response_model=List[ComplaintRecord], tags=["Analysis"])
def get_complaints_by_branch_code(branch_code: str):
    """
    Fetches all complaints for a specific branch.
    """
    analyzer = get_analyzer()
    records = analyzer.get_complaints_by_branch(branch_code)
    
    if not records:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"No complaints found for Branch Code '{branch_code}'."
        )
    
    return records

@app.get("/analysis/status/{status_value}", response_model=List[ComplaintRecord], tags=["Analysis"])
def get_complaints_by_status(status_value: str):
    """
    Fetches all complaints with a specific status.
    """
    analyzer = get_analyzer()
    records = analyzer.get_complaints_by_status(status_value)
    
    if not records:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"No complaints found with Status '{status_value}'."
        )
    
    return records

@app.get("/analysis/filter", response_model=List[ComplaintRecord], tags=["Analysis"])
def filter_complaints_records(
    category: Optional[str] = Query(None, description="Filter by complaint category."),
    columns: Optional[List[str]] = Query(None, description="Columns to return."),
    phone: Optional[str] = Query(None, description="Filter by phone number."),
    status: Optional[str] = Query(None, description="Filter by status.")
):
    """
    Advanced filtering with multiple criteria and column projection.
    """
    analyzer = get_analyzer()
    
    # Convert columns list to set
    columns_set = set(columns) if columns else None
    
    # Use the analyzer's filter method
    records = analyzer.filter_complaints(
        category=category,
        columns=columns_set,
        phone=phone,
        status=status
    )
    
    if not records:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No records found matching filter criteria."
        )
    
    return records

@app.get("/analysis/all-data", response_model=List[ComplaintRecord], tags=["Analysis"])
def get_all_complaints():
    """
    Get all complaints in the database.
    """
    analyzer = get_analyzer()
    df = analyzer.df
    
    if df is None or df.empty:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No complaints found in database."
        )
    
    records = df.to_dict('records')
    return records

@app.get("/analysis/columns/{column_name}", response_model=List[Any], tags=["Analysis"])
def get_unique_column_values(column_name: str):
    """
    Get all unique values for a specific column.
    """
    analyzer = get_analyzer()
    df = analyzer.df
    
    if df is None or df.empty:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No data available."
        )
    
    if column_name not in df.columns:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Column '{column_name}' not found in database."
        )
    
    unique_values = df[column_name].dropna().unique().tolist()
    return unique_values


# --- New Advanced Analysis Endpoints ---

@app.get("/analysis/statistics", response_model=StatisticsResponse, tags=["Analysis"])
def get_comprehensive_statistics():
    """
    Get comprehensive statistics including counts, averages, and breakdowns.
    """
    analyzer = get_analyzer()
    df = analyzer.df
    
    if df is None or df.empty:
        return StatisticsResponse(
            total_complaints=0,
            open_complaints=0,
            overtime_complaints=0,
            average_resolution_time="N/A",
            min_resolution_time="N/A",
            max_resolution_time="N/A",
            median_resolution_time="N/A",
            by_category={},
            by_status={}
        )
    
    total = len(df)
    
    # Get status breakdown
    status_col = None
    for col in ["status", "Status"]:
        if col in df.columns:
            status_col = col
            break
    
    by_status = df[status_col].value_counts().to_dict() if status_col and status_col in df.columns else {}
    open_count = by_status.get("Open", 0) + by_status.get("open", 0) + by_status.get("Pending", 0) + by_status.get("pending", 0)
    
    # Get category breakdown
    category_col = None
    for col in ["complaint_categories", "Category", "category"]:
        if col in df.columns:
            category_col = col
            break
    
    by_category = df[category_col].value_counts().to_dict() if category_col and category_col in df.columns else {}
    
    # Calculate resolution times
    resolution_times = []
    for _, row in df.iterrows():
        rt = calculate_resolution_time(row.to_dict())
        if rt is not None:
            resolution_times.append(rt)
    
    avg_resolution = f"{sum(resolution_times) / len(resolution_times):.1f} days" if resolution_times else "N/A"
    min_resolution = f"{min(resolution_times)} days" if resolution_times else "N/A"
    max_resolution = f"{max(resolution_times)} days" if resolution_times else "N/A"
    median_resolution = f"{sorted(resolution_times)[len(resolution_times)//2]} days" if resolution_times else "N/A"
    
    # Count overtime
    overtime_count = sum(1 for _, row in df.iterrows() if is_overtime(row.to_dict()))
    
    return StatisticsResponse(
        total_complaints=total,
        open_complaints=open_count,
        overtime_complaints=overtime_count,
        average_resolution_time=avg_resolution,
        min_resolution_time=min_resolution,
        max_resolution_time=max_resolution,
        median_resolution_time=median_resolution,
        by_category=by_category,
        by_status=by_status
    )

@app.get("/analysis/assignee/{assignee_name}", response_model=List[ComplaintRecord], tags=["Analysis"])
def get_assignee_complaints(assignee_name: str):
    """
    Get all complaints assigned to a specific person.
    """
    analyzer = get_analyzer()
    df = analyzer.df
    
    if df is None or df.empty:
        raise HTTPException(status_code=404, detail="No data available")
    
    assignee_col = None
    for col in ["assigned_officer", "Assignee", "assignee"]:
        if col in df.columns:
            assignee_col = col
            break
    
    if not assignee_col:
        raise HTTPException(status_code=404, detail="Assignee column not found")
    
    filtered = df[df[assignee_col].astype(str).str.lower().str.contains(assignee_name.lower(), na=False)]
    records = filtered.to_dict('records')
    
    if not records:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"No complaints found for assignee '{assignee_name}'."
        )
    
    return records

@app.get("/analysis/date-range", response_model=List[ComplaintRecord], tags=["Analysis"])
def get_date_range_complaints(
    start_date: str = Query(..., description="Start date (YYYY-MM-DD)"),
    end_date: str = Query(..., description="End date (YYYY-MM-DD)")
):
    """
    Get complaints within a specific date range.
    """
    analyzer = get_analyzer()
    df = analyzer.df
    
    start = parse_date(start_date)
    end = parse_date(end_date)
    
    if not start or not end:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid date format. Use YYYY-MM-DD"
        )
    
    if df is None or df.empty:
        raise HTTPException(status_code=404, detail="No data available")
    
    date_col = None
    for col in ["date_entry", "date_of_issue", "Entry Date", "Date", "entry_date"]:
        if col in df.columns:
            date_col = col
            break
    
    if not date_col:
        raise HTTPException(status_code=404, detail="Date column not found")
    
    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
    filtered = df[(df[date_col] >= start) & (df[date_col] <= end)]
    records = filtered.to_dict('records')
    
    if not records:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"No complaints found between {start_date} and {end_date}."
        )
    
    return records

@app.get("/analysis/overtime", response_model=List[ComplaintRecord], tags=["Analysis"])
def get_overtime_complaints(
    sla_days: int = Query(30, description="SLA threshold in days")
):
    """
    Get all complaints that exceeded SLA.
    """
    analyzer = get_analyzer()
    df = analyzer.df
    
    if df is None or df.empty:
        raise HTTPException(status_code=404, detail="No data available")
    
    overtime_complaints = []
    for _, row in df.iterrows():
        row_dict = row.to_dict()
        if is_overtime(row_dict, sla_days):
            row_dict['resolution_time_days'] = calculate_resolution_time(row_dict)
            overtime_complaints.append(row_dict)
    
    records = overtime_complaints
    
    if not records:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No overtime complaints found."
        )
    
    return records

class GenerateReportRequest(BaseModel):
    """Model for report generation request."""
    category: str
    records: List[Dict[str, Any]]

@app.post("/analysis/generate-report", tags=["Analysis"])
def generate_analysis_report(request: GenerateReportRequest):
    """
    Generate an AI analysis report for a category.
    """
    try:
        category = request.category
        records = request.records
        
        if not records or len(records) == 0:
            return {
                "report": f"Analysis Report for {category}\n\nNo records available for analysis."
            }
        
        # Create a simple analysis report
        report = f"""# Analysis Report: {category}

## Summary
- Total Records: {len(records)}
- Category: {category}

## Key Insights
- This report analyzes {len(records)} complaints in the {category} category
- Data includes various status types and dates for trend analysis

## Statistics
"""
        
        # Count statuses
        status_counts = {}
        for record in records:
            status = record.get('status', 'Unknown')
            status_counts[status] = status_counts.get(status, 0) + 1
        
        report += "\n### Status Breakdown\n"
        for status, count in status_counts.items():
            percentage = (count / len(records) * 100)
            report += f"- {status}: {count} ({percentage:.1f}%)\n"
        
        report += "\n### Recommendations\n"
        report += "- Monitor high-priority complaints closely\n"
        report += "- Ensure timely resolution of pending items\n"
        report += "- Review patterns in complaint categories\n"
        
        return {"report": report}
    except Exception as e:
        logger.error(f"Report generation error: {e}")
        return {"report": f"Error generating report: {str(e)}"}

@app.get("/analysis/resolution-time", response_model=ResolutionTimeResponse, tags=["Analysis"])
def get_resolution_time_analysis():
    """
    Analyze resolution times across all complaints.
    """
    analyzer = get_analyzer()
    df = analyzer.df
    
    if df is None or df.empty:
        return ResolutionTimeResponse(
            average_resolution_time="N/A",
            min_resolution_time="N/A",
            max_resolution_time="N/A",
            median_resolution_time="N/A",
            total_resolved=0
        )
    
    resolution_times = []
    for _, row in df.iterrows():
        rt = calculate_resolution_time(row.to_dict())
        if rt is not None:
            resolution_times.append(rt)
    
    if not resolution_times:
        return ResolutionTimeResponse(
            average_resolution_time="N/A",
            min_resolution_time="N/A",
            max_resolution_time="N/A",
            median_resolution_time="N/A",
            total_resolved=0
        )
    
    sorted_times = sorted(resolution_times)
    
    return ResolutionTimeResponse(
        average_resolution_time=f"{sum(resolution_times) / len(resolution_times):.1f} days",
        min_resolution_time=f"{min(resolution_times):.1f} days",
        max_resolution_time=f"{max(resolution_times):.1f} days",
        median_resolution_time=f"{sorted_times[len(sorted_times)//2]:.1f} days",
        total_resolved=len(resolution_times)
    )